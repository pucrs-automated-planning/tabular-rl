{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d99a971c",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pucrs-automated-planning/tabular-rl/blob/main/src/Tabular-Methods.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2fb881b",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Tutorial 02: Tabular Methods\n",
    "\n",
    "#### Prof. Felipe Meneguzzi\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this practical, you will implement two tabular methods of reinforcement learning: [Q-Learning](https://en.wikipedia.org/wiki/Q-learning) and [Sarsa](https://en.wikipedia.org/wiki/State–action–reward–state–action) according to Sutton and Barto's book. Our implementation will be based on a simple [RLAgent](rl/agent.py) class provided by the instructor, and the simplest of the [Gym](https://github.com/openai/gym) environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdfdd6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "You will need to configure your environment following the instructions in the `README.md` file included in this repository.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5478e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  print(\"We are in Google colab, we need to clone the repo\")\n",
    "  !git clone https://github.com/pucrs-automated-planning/tabular-rl.git\n",
    "  %cd tabular-rl\n",
    "  %pip install -r requirements.txt\n",
    "  %cd src\n",
    "except:\n",
    "  print(\"Not in colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c5c24",
   "metadata": {},
   "source": [
    "## The agent interface\n",
    "\n",
    "In this notebook we will use a basic agent interface called [`RLAgent`](src/rl/agent.py). Open this file and become familiar with its key attributes and methods. \n",
    "\n",
    "The key attributes you will need for this notebook are the following:\n",
    "- `self.env` — a reference to the [environment from gym](https://github.com/openai/gym/blob/58ed658d9b15fd410c50d1fdb25a7cad9acb7fa4/gym/core.py#L8);\n",
    "- `self.episodes` — the number of episodes to train the agent when invoking [`RLAgent.learn()`](rl/agent.py#L125);\n",
    "- `self.alpha` — the *learning rate* to be used when computing the RL update;\n",
    "- `self.gamma` — the *discount factor* for the return computed during training; and\n",
    "- `self.q_table` — a dictionary of lists of floating point numbers indexed by the state and an action (e.g., `self.q_table[state][action]` should return the q-value of the `state`-`action` pair)\n",
    "- `self.actions` — an integer representing the size of the action space\n",
    "- `self.last_state` — the last state visited by the agent during learning\n",
    "- `self.last_action` — the last action chosen by the agent during learning\n",
    "- `self._random` — a reference to a Python [Random](https://docs.python.org/3/library/random.html#random.Random) object.\n",
    "\n",
    "Pay attention to the `self.random` object, and use it instead of the unbound methods from Python's [random library](https://docs.python.org/3/library/random.html), otherwise the tests will not work. \n",
    "\n",
    "The key methods you should be aware of are the following:\n",
    "- [`policy`](rl/agent.py#L90) — returns the greedy policy following the currently learned q-values;\n",
    "- [`epsilon_greedy_policy`](rl/agent.py#L101) — returns an $\\epsilon$-greedy policy;\n",
    "- [`agent_start`](rl/agent.py#L55) — the method to be called once *before each episode*;\n",
    "- [`agent_step`](rl/agent.py#L67) — the method to be called once at *each time step* within an episode;\n",
    "- [`agent_end`](rl/agent.py#L80) — the method to be called once at the *end of each episode* when the agent reaches a *terminal state*; and\n",
    "- [`learn`](rl/agent.py#L125) — runs the underlying learning algorithm for `self.episodes` episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1fb89",
   "metadata": {},
   "source": [
    "## Greedy and $\\epsilon$-Greedy Policies\n",
    "\n",
    "In the cell below, you need to implement both the greedy and the $\\epsilon$-greedy policy using the q-values stored in `self.q_table` and the $\\epsilon$ from `self.eps()`. \n",
    "\n",
    "Recall the definition of $\\epsilon$-greedy policy:\n",
    "\n",
    "For all $a \\in \\mathcal{A}$\n",
    "$$\\pi(a \\mid S_t) = \\begin{cases} 1 - \\epsilon +\\frac{\\epsilon}{|\\mathcal{A}|} & \\text{if}~a = A^* \\\\ \\frac{\\epsilon}{|\\mathcal{A}|} & \\text{if}~a \\neq A^*\\end{cases}$$\n",
    "where $A^* = \\arg\\max_{a}Q(S_t,a)$, with ties broken randomly (this is the greedy policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625dd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from rl.agent import RLAgent, Action, State\n",
    "from random import Random\n",
    "from gym import Env\n",
    "from typing import Any, Collection, List, NoReturn, Sequence\n",
    "\n",
    "class TabularQLearner(RLAgent):\n",
    "    \"\"\"\n",
    "    A simple Tabular Q-Learning agent.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env: Env,\n",
    "                 episodes: int = 500,\n",
    "                 decaying_eps: bool = True,\n",
    "                 eps: float = 1.0,\n",
    "                 alpha: float = 0.5,\n",
    "                 decay: float = 0.000002,\n",
    "                 gamma: float = 0.9,\n",
    "                 rand: Random = Random(),\n",
    "                 **kwargs):\n",
    "        super().__init__(env, episodes=episodes, decaying_eps=decaying_eps, eps=eps, alpha=alpha, decay=decay, gamma=gamma, rand=rand)\n",
    "        self.actions = env.action_space.n\n",
    "        self.q_table = {}\n",
    "\n",
    "        # hyperparameters\n",
    "        self.episodes = episodes\n",
    "        self.gamma = gamma\n",
    "        self.decay = decay\n",
    "        self.c_eps = eps\n",
    "        self.base_eps = eps\n",
    "        if decaying_eps:\n",
    "            def epsilon():\n",
    "                self.c_eps = max((self.episodes - self.step)/self.episodes, 0.01)\n",
    "                return self.c_eps\n",
    "            self.eps = epsilon\n",
    "        else:\n",
    "            self.eps = lambda: eps\n",
    "        self.decaying_eps = decaying_eps\n",
    "        self.alpha = alpha\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "    \n",
    "    def add_new_state(self, state: State):\n",
    "        # self.q_table[state] = [1. for _ in range(self.actions)]\n",
    "        self.q_table[state] = [0.]*self.actions\n",
    "\n",
    "    def get_all_q_values(self, state: State) -> List[float]:\n",
    "        if state in self.q_table:\n",
    "            return self.q_table[state]\n",
    "        else:\n",
    "            return [0.]*self.actions\n",
    "\n",
    "    def best_actions(self, state: State) -> Sequence[int]:\n",
    "        \"\"\"Returns a list with the best actions for a particular state\n",
    "\n",
    "        Args:\n",
    "            state (State): The state for which we need the best actions\n",
    "\n",
    "        Returns:\n",
    "            Sequence[int]: a list with the best actions\n",
    "        \"\"\"\n",
    "        if state not in self.q_table:\n",
    "            self.add_new_state(state)\n",
    "        q_next = np.array(self.q_table[state])\n",
    "        best_actions = np.argwhere(q_next == np.max(q_next)).flatten()\n",
    "        return best_actions\n",
    "\n",
    "    def best_action(self, state: State) -> Action:\n",
    "        \"\"\"Returns the best action for a state, breaking ties randomly\n",
    "\n",
    "        Args:\n",
    "            state (State): The state in which to extract the best action\n",
    "\n",
    "        Returns:\n",
    "            int: The index of the best action\n",
    "        \"\"\"\n",
    "        if state not in self.q_table:\n",
    "            self.add_new_state(state)\n",
    "            # self.q_table[state] = [0 for _ in range(self.actions)]\n",
    "        # return np.argmax(self.q_table[state])\n",
    "        best_actions = self.best_actions(state)\n",
    "\n",
    "        return self._random.choice(best_actions)\n",
    "    \n",
    "    def policy(self, state: State) -> Action:\n",
    "        \"\"\"Returns the greedy deterministic policy for the specified state\n",
    "\n",
    "        Args:\n",
    "            state (State): the state for which we want the action\n",
    "\n",
    "        Raises:\n",
    "            InvalidAction: Not sure about this one\n",
    "\n",
    "        Returns:\n",
    "            Any: The greedy action learned for state\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (1 line)\n",
    "        pass\n",
    "        ## END CODE\n",
    "\n",
    "    def epsilon_greedy_policy(self, state: State) -> Action:\n",
    "        \"\"\"Returns the epsilon-greedy policy\n",
    "\n",
    "        Args:\n",
    "            state (State): The state for which to return the epsilon greedy policy\n",
    "\n",
    "        Returns:\n",
    "            Any: The action to be taken\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (6 lines)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        ## END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03117f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "agent = TabularQLearner(env = env, decaying_eps=False, eps=0.5, rand = Random(42))\n",
    "agent.q_table[\"a\"] = [1, 0.5, 4, 0]\n",
    "agent.q_table[\"b\"] = [0, 1, 0, 0.5]\n",
    "\n",
    "action_a = agent.epsilon_greedy_policy(\"a\")\n",
    "action_b = agent.epsilon_greedy_policy(\"b\")\n",
    "print(f\"Action for a={action_a}\")\n",
    "print(f\"Action for b={action_b}\")\n",
    "\n",
    "assert(action_a == 2)\n",
    "assert(action_b == 1)\n",
    "\n",
    "action_a = agent.epsilon_greedy_policy(\"a\")\n",
    "action_b = agent.epsilon_greedy_policy(\"b\")\n",
    "\n",
    "print(f\"Action for a={action_a}\")\n",
    "print(f\"Action for b={action_b}\")\n",
    "\n",
    "assert(action_a == 0)  # With our random seed, this one should be exploratory\n",
    "assert(action_b == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9c9d2",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "**Description** \n",
    "\n",
    "Q-Learning is an off policy algorithm that learns the target policy by temporal-difference from a greedy target policy using the following update rule. In our implementation, we will use the $\\epsilon$-greedy policy as the source policy. \n",
    "\n",
    "$$ Q(S,A) \\gets Q(S,A) + \\alpha \\left(R + \\gamma\\max_{a'}Q(S',a') - Q(S,A) \\right) $$ \n",
    "\n",
    "In the cell below, you should implement the following methods (as noted in the source code):\n",
    "- `policy` and `epsilon_greedy_policy` - copy the code you just developed above\n",
    "- `agent_start` - initialize the last state with the initial state given by the environment and choose one initial action\n",
    "- `agent_step` - this should follow the algorithm in Section 6.5 from Sutton and Barto\n",
    "- `agent_end` - one last update at the end of the episode, but using just the old value of Q as the TD-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524957f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQLearner(RLAgent):\n",
    "    \"\"\"\n",
    "    A simple Tabular Q-Learning agent.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 env: Env,\n",
    "                 episodes: int = 500,\n",
    "                 decaying_eps: bool = True,\n",
    "                 eps: float = 1.0,\n",
    "                 alpha: float = 0.5,\n",
    "                 decay: float = 0.000002,\n",
    "                 gamma: float = 0.9,\n",
    "                 rand: Random = Random(),\n",
    "                 **kwargs):\n",
    "        super().__init__(env, episodes=episodes, decaying_eps=decaying_eps, eps=eps, alpha=alpha, decay=decay, gamma=gamma, rand=rand)\n",
    "        self.actions = env.action_space.n\n",
    "        self.q_table = {}\n",
    "\n",
    "        # hyperparameters\n",
    "        self.episodes = episodes\n",
    "        self.gamma = gamma\n",
    "        self.decay = decay\n",
    "        self.c_eps = eps\n",
    "        self.base_eps = eps\n",
    "        if decaying_eps:\n",
    "            def epsilon():\n",
    "                self.c_eps = max((self.episodes - self.step)/self.episodes, 0.01)\n",
    "                return self.c_eps\n",
    "            self.eps = epsilon\n",
    "        else:\n",
    "            self.eps = lambda: eps\n",
    "        self.decaying_eps = decaying_eps\n",
    "        self.alpha = alpha\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def add_new_state(self, state: State):\n",
    "        # self.q_table[state] = [1. for _ in range(self.actions)]\n",
    "        self.q_table[state] = [0.]*self.actions\n",
    "\n",
    "    def get_all_q_values(self, state: State) -> List[float]:\n",
    "        if state in self.q_table:\n",
    "            return self.q_table[state]\n",
    "        else:\n",
    "            return [0.]*self.actions\n",
    "\n",
    "    def best_actions(self, state: State) -> Sequence[int]:\n",
    "        \"\"\"Returns a list with the best actions for a particular state\n",
    "\n",
    "        Args:\n",
    "            state (State): The state for which we need the best actions\n",
    "\n",
    "        Returns:\n",
    "            Sequence[int]: a list with the best actions\n",
    "        \"\"\"\n",
    "        if state not in self.q_table:\n",
    "            self.add_new_state(state)\n",
    "        q_next = np.array(self.q_table[state])\n",
    "        best_actions = np.argwhere(q_next == np.max(q_next)).flatten()\n",
    "        return best_actions\n",
    "\n",
    "    def best_action(self, state: State) -> Action:\n",
    "        \"\"\"Returns the best action for a state, breaking ties randomly\n",
    "\n",
    "        Args:\n",
    "            state (State): The state in which to extract the best action\n",
    "\n",
    "        Returns:\n",
    "            int: The index of the best action\n",
    "        \"\"\"\n",
    "        if state not in self.q_table:\n",
    "            self.add_new_state(state)\n",
    "            # self.q_table[state] = [0 for _ in range(self.actions)]\n",
    "        # return np.argmax(self.q_table[state])\n",
    "        best_actions = self.best_actions(state)\n",
    "\n",
    "        return self._random.choice(best_actions)\n",
    "    \n",
    "    def get_max_q(self, state: State) -> float:\n",
    "        if state not in self.q_table:\n",
    "            self.add_new_state(state)\n",
    "        return np.max(self.q_table[state])\n",
    "    \n",
    "    def set_q_value(self, state: State, action: Any, q_value: float):\n",
    "        if state not in self.q_table:\n",
    "            self.add_new_state(state)\n",
    "        self.q_table[state][action] = q_value\n",
    "\n",
    "    def get_q_value(self, state: State, action: Any) -> float:\n",
    "        if state not in self.q_table:\n",
    "            self.add_new_state(state)\n",
    "        return self.q_table[state][action]\n",
    "\n",
    "    \n",
    "    def policy(self, state: State) -> Action:\n",
    "        \"\"\"Returns the greedy deterministic policy for the specified state\n",
    "\n",
    "        Args:\n",
    "            state (State): the state for which we want the action\n",
    "\n",
    "        Raises:\n",
    "            InvalidAction: Not sure about this one\n",
    "\n",
    "        Returns:\n",
    "            Any: The greedy action learned for state\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (1 line)\n",
    "        pass\n",
    "        ## END CODE\n",
    "\n",
    "    def epsilon_greedy_policy(self, state: State) -> Action:\n",
    "        \"\"\"Returns the epsilon-greedy policy\n",
    "\n",
    "        Args:\n",
    "            state (State): The state for which to return the epsilon greedy policy\n",
    "\n",
    "        Returns:\n",
    "            Any: The action to be taken\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (6 lines)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        ## END CODE\n",
    "    \n",
    "    def agent_start(self, state: State) -> Action:\n",
    "        \"\"\"The first method called when the experiment starts,\n",
    "        called after the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's env_start function.\n",
    "        Returns:\n",
    "            (int) the first action the agent takes.\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (3 lines)\n",
    "        \n",
    "        \n",
    "        pass\n",
    "        ## END CODE\n",
    "\n",
    "    def agent_step(self, reward: float, state: State) -> Action:\n",
    "        \"\"\"A step taken by the agent.\n",
    "\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Any): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            (int) The action the agent takes given this state.\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (9 lines)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        ## END CODE\n",
    "\n",
    "    def agent_end(self, reward: float) -> NoReturn:\n",
    "        \"\"\"Called when the agent terminates.\n",
    "\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (4 lines)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        ## END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Q-Learning\n",
    "## Mini test\n",
    "nv = gym.make(\"CliffWalking-v0\")\n",
    "agent = TabularQLearner(env = env, decaying_eps=False, eps=0.5, rand = Random(42))\n",
    "agent.q_table[\"a\"] = [1, 0.5, 4, 0]\n",
    "agent.q_table[\"b\"] = [0, 1, 0, 0.5]\n",
    "\n",
    "action_a = agent.agent_start(\"a\")\n",
    "assert(action_a == 2)\n",
    "\n",
    "action_a = agent.agent_step(-1, \"a\")\n",
    "# print(action_a)\n",
    "# print(agent.q_table[\"a\"])\n",
    "# print(agent.q_table[\"b\"])\n",
    "assert(action_a == 2)\n",
    "assert(agent.q_table[\"a\"] == [1, 0.5, 3.3, 0])\n",
    "assert(agent.q_table[\"b\"] == [0, 1, 0, 0.5])\n",
    "\n",
    "action_b = agent.agent_step(-2, \"b\")\n",
    "# print(action_b)\n",
    "# print(agent.q_table[\"a\"])\n",
    "# print(agent.q_table[\"b\"])\n",
    "assert(action_b == 0)\n",
    "assert(np.where(np.isclose(agent.q_table[\"a\"], [1, 0.5, 1.1, 0])))\n",
    "assert(agent.q_table[\"b\"] == [0, 1, 0, 0.5])\n",
    "\n",
    "action_b = agent.agent_step(1, \"b\")\n",
    "# print(action_b)\n",
    "# print(agent.q_table[\"a\"])\n",
    "# print(agent.q_table[\"b\"])\n",
    "assert(action_b == 1)\n",
    "assert(np.where(np.isclose(agent.q_table[\"a\"], [1, 0.5, 1.1, 0])))\n",
    "assert(agent.q_table[\"b\"] == [0.95, 1, 0, 0.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336da19a",
   "metadata": {},
   "source": [
    "## The Cliff Walking Environment\n",
    "We are now ready to learn our first real policy for a non-trivial environment. At the end of training, we print the resulting policy. \n",
    "\n",
    "The environment we use is the Cliff Walking environment from Example 6.6 in Sutton and Barto's book. Refer to page 132 for further details, but we post a summary of the environment below for your convenience.\n",
    "![\"Cliff Sarsa\"](cliff-sarsa.svg)\n",
    "\n",
    "Why does the policy look the way it does (i.e., a bit random at times)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff87e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "from rl.env_utils import display_cliffwalking_policy\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "q_agent = TabularQLearner(env, episodes=500, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False, rand=Random(42))\n",
    "rewards_q = q_agent.learn()\n",
    "\n",
    "print(f\"Accumulated Reward: {np.sum(rewards_q)}\")\n",
    "# assert(np.sum(rewards_q) == -28481)\n",
    "display_cliffwalking_policy(q_agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454ef7e",
   "metadata": {},
   "source": [
    "## Sarsa\n",
    "\n",
    "Sarsa is an on policy algorithm where the source and target policy are the same. In our implementation, we will use the $\\epsilon$-greedy policy we implemented above.\n",
    "\n",
    "$$ Q(S,A) \\gets Q(S,A) + \\alpha \\left(R + \\gamma Q(S',A') - Q(S,A) \\right) $$ \n",
    "\n",
    "Note that we can implement Sarsa by simply subclassing the `TabularQLearner` class we just implemented. This time, the only modification we need is in the `agent_step` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7072aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularSarsaLearner(TabularQLearner):\n",
    "    def agent_step(self, reward: float, state: State) -> Action:\n",
    "        \"\"\"A step taken by the agent.\n",
    "\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Any): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            (int) The action the agent takes given this state.\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (9 lines)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        ## END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927675f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Sarsa\n",
    "\n",
    "v = gym.make(\"CliffWalking-v0\")\n",
    "sarsa_agent = TabularSarsaLearner(env = env, decaying_eps=False, eps=0.5, rand = Random(42))\n",
    "sarsa_agent.q_table[\"a\"] = [1, 0.5, 4, 0]\n",
    "sarsa_agent.q_table[\"b\"] = [0, 1, 0, 0.5]\n",
    "\n",
    "action_a = sarsa_agent.agent_start(\"a\")\n",
    "assert(action_a == 2)\n",
    "\n",
    "action_a = sarsa_agent.agent_step(-1, \"a\")\n",
    "# print(action_a)\n",
    "# print(sarsa_agent.q_table[\"a\"])\n",
    "# print(sarsa_agent.q_table[\"b\"])\n",
    "assert(action_a == 2)\n",
    "assert(sarsa_agent.q_table[\"a\"] == [1, 0.5, 3.3, 0])\n",
    "assert(sarsa_agent.q_table[\"b\"] == [0, 1, 0, 0.5])\n",
    "\n",
    "action_b = sarsa_agent.agent_step(-2, \"b\")\n",
    "# print(action_b)\n",
    "# print(sarsa_agent.q_table[\"a\"])\n",
    "# print(sarsa_agent.q_table[\"b\"])\n",
    "assert(action_b == 0)\n",
    "assert(np.where(np.isclose(sarsa_agent.q_table[\"a\"], [1, 0.5, 1.1, 0])))\n",
    "assert(sarsa_agent.q_table[\"b\"] == [0, 1, 0, 0.5])\n",
    "\n",
    "action_b = sarsa_agent.agent_step(1, \"b\")\n",
    "# print(action_b)\n",
    "# print(sarsa_agent.q_table[\"a\"])\n",
    "# print(sarsa_agent.q_table[\"b\"])\n",
    "assert(action_b == 1)\n",
    "assert(np.where(np.isclose(sarsa_agent.q_table[\"a\"], [1, 0.5, 1.1, 0])))\n",
    "assert(sarsa_agent.q_table[\"b\"] == [0.95, 1, 0, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c4d52",
   "metadata": {},
   "source": [
    "## Expected Sarsa\n",
    "\n",
    "Expected Sarsa is a variation of the Sarsa algorithm where, instead of computing the TD target using the actual samples from a stochastic source policy, we compute the expectation from that policy, much like the update rule in Dynamic Programming, using the following update\n",
    "\n",
    "$$ Q(S,A) \\gets Q(S,A) + \\alpha \\left(R + \\gamma \\sum_{a'}\\pi(a'\\mid S')Q(S',a') - Q(S,A) \\right) $$ \n",
    "\n",
    "Like Sarsa, we can implement Expected Sarsa by simply subclassing the `TabularQLearner` class we just implemented. This time, the only modification we need is in the `agent_step` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularExpectedSarsaLearner(TabularQLearner):\n",
    "    def agent_step(self, reward: float, state: State) -> Action:\n",
    "        \"\"\"A step taken by the agent.\n",
    "\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Any): the state from the\n",
    "                environment's step based on where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            (int) The action the agent takes given this state.\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE (17 lines)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pass\n",
    "        ## END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e055f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Expected Sarsa\n",
    "\n",
    "v = gym.make(\"CliffWalking-v0\")\n",
    "expected_sarsa_agent = TabularExpectedSarsaLearner(env = env, decaying_eps=False, eps=0.5, rand = Random(42))\n",
    "expected_sarsa_agent.q_table[\"a\"] = [1, 0.5, 4, 0]\n",
    "expected_sarsa_agent.q_table[\"b\"] = [0, 1, 0, 0.5]\n",
    "\n",
    "action_a = expected_sarsa_agent.agent_start(\"a\")\n",
    "assert(action_a == 2)\n",
    "\n",
    "action_a = expected_sarsa_agent.agent_step(-1, \"a\")\n",
    "print(action_a)\n",
    "print(expected_sarsa_agent.q_table[\"a\"])\n",
    "print(expected_sarsa_agent.q_table[\"b\"])\n",
    "assert(action_a == 2)\n",
    "assert(np.where(np.isclose(expected_sarsa_agent.q_table[\"a\"], [1, 0.5, 2.7, 0])))\n",
    "assert(expected_sarsa_agent.q_table[\"b\"] == [0, 1, 0, 0.5])\n",
    "\n",
    "action_b = sarsa_agent.agent_step(-2, \"b\")\n",
    "print(action_b)\n",
    "print(expected_sarsa_agent.q_table[\"a\"])\n",
    "print(expected_sarsa_agent.q_table[\"b\"])\n",
    "assert(action_b == 1)\n",
    "assert(np.where(np.isclose(expected_sarsa_agent.q_table[\"a\"], [1, 0.5, 2.7, 0])))\n",
    "assert(expected_sarsa_agent.q_table[\"b\"] == [0, 1, 0, 0.5])\n",
    "\n",
    "action_b = expected_sarsa_agent.agent_step(1, \"b\")\n",
    "print(action_b)\n",
    "print(agent.q_table[\"a\"])\n",
    "print(agent.q_table[\"b\"])\n",
    "assert(action_b == 0)\n",
    "assert(np.where(np.isclose(expected_sarsa_agent.q_table[\"a\"], [1, 0.5, 1.1, 0])))\n",
    "assert(np.where(np.isclose(expected_sarsa_agent.q_table[\"b\"], [0.95, 1, 0, 0.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Learning Methods\n",
    "\n",
    "Our final experiment compares the accumulated rewards for each of our TD learning algorithms. This experiment replicates exactly example 6.6 (plus Expected Sarsa) in the book by plotting the cumulative rewards per time step of training each of our algorithms achieves in the Cliff Walking environment. \n",
    "\n",
    "If your implementation is correct, you should see a graph like the one below:\n",
    "![\"Cumulative Rewards\"](figure_example_6_6.svg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "test_episodes = 500\n",
    "runs = 50\n",
    "rewards_q = np.zeros(test_episodes)\n",
    "rewards_sarsa = np.zeros(test_episodes)\n",
    "rewards_expected_sarsa = np.zeros(test_episodes)\n",
    "for i in range(runs):\n",
    "    q_agent = TabularQLearner(env, episodes=test_episodes, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False)\n",
    "    sarsa_agent = TabularSarsaLearner(env, episodes=test_episodes, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False)\n",
    "    expected_sarsa_agent = TabularExpectedSarsaLearner(env, episodes=test_episodes, gamma=1, eps=0.1, alpha=0.5, decaying_eps=False)\n",
    "    rewards_q += q_agent.learn()\n",
    "    rewards_sarsa += sarsa_agent.learn()\n",
    "    rewards_expected_sarsa += expected_sarsa_agent.learn()\n",
    "\n",
    "rewards_q /= runs\n",
    "rewards_sarsa /= runs\n",
    "rewards_expected_sarsa /= runs\n",
    "\n",
    "# draw reward curves\n",
    "plt.plot(rewards_sarsa, label='Sarsa')\n",
    "plt.plot(rewards_q, label='Q-Learning')\n",
    "plt.plot(rewards_expected_sarsa, label='Expected Sarsa')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Sum of rewards during episode')\n",
    "plt.ylim([-100, 0])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e181e91a4207cd28265f13374a7fae83cda3ae570825d0fda8366d350b98fe83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
